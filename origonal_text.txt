Linear Regression from First Principle | by Ibrahim Salihu Yusuf | MediumIbrahim Salihu YusufFollowOct 29, 2019·6 min readLinear Regression from First PrincipleLinear regression is the “hello world” of machine learning. In this article we will go through the mathematics behind linear regression then code it from scratch. Little familiarity of Numpy and Pandas shall be assumed.In machine learning, a linear regression problem requires training a model that will predict continuous numeric values from a set of input values. So that given a set of input values, which are called features, a linear regression model outputs a value which is referred to as the target. The task in linear regression is to form a hypothesis which will, to a good level of accuracy, output the target from a set of input features. Mathematically, this hypothesis can be written as,where w1, w2 are the weights; x1, x2 are the input features; and b is the bias.To develop a linear regression model based on the hypothesis of equation (1), we need to learn from a collection of input features (training examples), the optimal values of weights and bias that will transform the inputs to a target. This process of learning from training examples is called training. The result of training is a model with optimal values of weights that can transform our input features to a predicted target.In training our linear regression model we will make use of the Boston housing prices data. This data shows the value of houses based on different features such as the average number of rooms per dwelling, the per capita crime rate by town, the proportion of owner-occupied units built prior to 1940 and so on. The problem at hand is to train a model that will predict the median value of owner-occupied homes given a set of input features. We will start by developing a hypothesis based on a single feature and later add more features. Let us propose a hypothesis about the value of houses based on the average number of rooms per dwelling. It is a good practice to visualize the data one is working with. Plotting the average number of rooms against the house values suggest a linear relationship as shown in figure 1.Figure 1: A plot of input feature versus targetRemember that we are not out to come up with a 100% accurate hypothesis, but one that can generalize with an acceptable level of accuracy. Based on this, we can assume a linear relationship as shown by the red line in figure 1, which can be modelled by equation (2).The input feature, in this case the average number of rooms, x1 is the independent variable in equation 2. We will learn the optimal values for w1 and b from the set of input examples we have. In machine learning, we start by assuming values for w1 and b then we evaluate our hypothesis and compare the predicted output with the actual output (target). The difference between the actual versus the predicted outputs is the cost (loss) with which we determine how optimal the values of w1 and b are, but in order to attain optimal values we need a rule that we will follow to adjust our weight and bias until we arrive at optimal values. There are many such rules that can be followed, but in machine learning the most commonly used is the gradient descent algorithm.Gradient DescentThe gradient descent algorithm works with a differentiable cost function and computes an update value for each weight and bias for every step of the gradient descent algorithm. So that each step gives the weights and bias a nudge in the direction of minimum cost. Training a linear regression model using gradient descent can be achieved by the following steps:Randomly initialize the weightsEvaluate the hypothesis to get a predicted outputCompute the cost i.e evaluate the cost functionCompute the gradients, which can be understood to be the contribution of each weight and bias to the resultant cost in step 3Update the weightsLet us go through these steps mathematically.Having seen how a single step of gradient descent is performed mathematically. Let us see how it can be coded in Python using Numpy and Pandas library.Linear Regression CodeWe shall be working with Jupyter Notebook with Numpy and Pandas library installed. In order to have this, you can simply install Anaconda.Let us start by importing the libraries we will be working withThen we read in our dataset and fetch the input features and targets. We also add the coefficient of the bias, which is 1. This is because we want to have the input features and bias in a single matrix.Now we have our set of input features and the coefficient of the bias as shown below:To begin with, we initialize our weights to zero and this is achieved by the code snippet belowThe result is a vector with two weights, one for the input feature and the other for the bias.The following code snippet illustrate the remaining stepsNow we have completed a single step of the gradient descent. However, a single step cannot get us optimal weights, we will have to iterate for a number of steps to achieve optimal values. The most important thing to take note about gradient descent is that for each step the cost must be decreasing to achieve convergence. We can either train for a predetermined number of steps or stop when convergence has been reached, that is when the cost no longer decreases.To train for a number of steps we initialize our training hyper-parameters: number of steps and learning rate, then train as illustrated in the following code snippet.After training for 100 steps, the graph in figure 2 reveals how the cost was reducing while training. We can see that after a number of gradient descent steps the loss remained nearly constant, at this point we say training has converged. In other words, optimal values for w1 and b has been achieved.Figure 2: Graph of training losses versus number of stepsWith this we have successfully trained our first linear regression model and we can see it converged. The Jupyter notebook for this article can be found here. Although this is not a practical linear regression model, remember that we only used one feature which is the average number of rooms. In a subsequent article we will train our model using more features.Thank you for reading.Machine LearningLinear Regression----More from Ibrahim Salihu YusufFollowLove podcasts or audiobooks? Learn on the go with our new app.Try KnowableRecommended from MediumChanuka JayasankainAnalytics VidhyaGet to know about Cluster AnalysisAndrew LoaderImpact of Machine Learning on Optimization & PersonalizationPrashant DandriyalAI Beginners’ Playground: Why You Shuffling Your Numeric Data Matters.Anuradha WickramarachchiinTowards Data ScienceVariational Autoencoders and BioinformaticsAvinash NavlaniinThe StartupRecommendation System for Streaming PlatformsMandy Gu“SeeFood”: Creating a Binary Classifier using Transfer LearningDeepak JaininApplied Machine LearningDimensionality Reduction and Visualization - Part-4 - tSNELars NielseninTowards Data ScienceA pictorial guide to understanding Random Forest AlgorithmAboutHelpTermsPrivacyGet the Medium appGet unlimited accessIbrahim Salihu Yusuf7 FollowersFollowHelpStatusWritersBlogCareersPrivacyTermsAboutText to speech


































